// NOTE: Assertions have been autogenerated by utils/generate-test-checks.py

// RUN: hlo_to_llvm_ir %s | FileCheck %s
// CHECK: define void @fusion(i8* noalias align 16 dereferenceable(8192) %[[VAL_0:.*]], i8* noalias align 16 dereferenceable(4) %[[VAL_3:.*]], i8* noalias align 16 dereferenceable(4) %[[VAL_4:.*]], i8* noalias align 64 dereferenceable(256) %[[VAL_1:.*]], i8* noalias align 64 dereferenceable(256) %[[VAL_2:.*]]) {
// CHECK-LABEL: entry:
// CHECK:         %[[VAL_0:.*]] = getelementptr inbounds i8, i8* %[[VAL_1:.*]], i64 0
// CHECK:         %[[VAL_2:.*]] = bitcast i8* %[[VAL_0]] to [2 x [32 x [32 x float]]]*
// CHECK:         %[[VAL_3:.*]] = getelementptr inbounds i8, i8* %[[VAL_4:.*]], i64 0
// CHECK:         %[[VAL_5:.*]] = bitcast i8* %[[VAL_3]] to float*
// CHECK:         %[[VAL_6:.*]] = getelementptr inbounds i8, i8* %[[VAL_7:.*]], i64 0
// CHECK:         %[[VAL_8:.*]] = bitcast i8* %[[VAL_6]] to float*
// CHECK:         %[[VAL_9:.*]] = getelementptr inbounds i8, i8* %[[VAL_10:.*]], i64 0
// CHECK:         %[[VAL_11:.*]] = bitcast i8* %[[VAL_9]] to [2 x [32 x float]]*
// CHECK:         %[[VAL_12:.*]] = getelementptr inbounds i8, i8* %[[VAL_13:.*]], i64 0
// CHECK:         %[[VAL_14:.*]] = bitcast i8* %[[VAL_12]] to [2 x [32 x float]]*
// CHECK:         %[[VAL_15:.*]] = call i32 @llvm.nvvm.read.ptx.sreg.ctaid.x(), !range !6
// CHECK:         %[[VAL_16:.*]] = zext i32 %[[VAL_15]] to i64
// CHECK:         %[[VAL_17:.*]] = call i32 @llvm.nvvm.read.ptx.sreg.tid.x(), !range !7
// CHECK:         %[[VAL_18:.*]] = zext i32 %[[VAL_17]] to i64
// CHECK:         %[[VAL_19:.*]] = mul nuw nsw i64 %[[VAL_16]], 64
// CHECK:         %[[VAL_20:.*]] = add nuw nsw i64 %[[VAL_19]], %[[VAL_18]]
// CHECK:         %[[VAL_21:.*]] = icmp ult i64 %[[VAL_20]], 64
// CHECK:         call void @llvm.assume(i1 %[[VAL_21]])
// CHECK:         %[[VAL_22:.*]] = udiv i64 %[[VAL_20]], 1
// CHECK:         %[[VAL_23:.*]] = urem i64 %[[VAL_22]], 32
// CHECK:         %[[VAL_24:.*]] = udiv i64 %[[VAL_20]], 32
// CHECK:         %[[VAL_25:.*]] = icmp ult i64 %[[VAL_20]], 64
// CHECK:         br i1 %[[VAL_25]], label %[[VAL_26:.*]], label %[[VAL_27:.*]]
// CHECK:       fusion.in_bounds-after:                           ; preds = %[[VAL_26]], %[[VAL_28:.*]]
// CHECK:         ret void
// CHECK:       fusion.in_bounds-true:                            ; preds = %[[VAL_28]]
// CHECK:         %[[VAL_29:.*]] = load float, float* %[[VAL_5]], align 4, !invariant.load !8
// CHECK:         %[[VAL_30:.*]] = bitcast [2 x [32 x float]]* %[[VAL_11]] to float*
// CHECK:         %[[VAL_31:.*]] = getelementptr inbounds float, float* %[[VAL_30]], i64 %[[VAL_20]]
// CHECK:         store float %[[VAL_29]], float* %[[VAL_31]], align 4
// CHECK:         br label %[[VAL_27]]

// CHECK: define void @fusion__1(i8* noalias align 16 dereferenceable(8192) %[[VAL_0:.*]], i8* noalias align 16 dereferenceable(4) %[[VAL_3:.*]], i8* noalias align 16 dereferenceable(4) %[[VAL_4:.*]], i8* noalias align 64 dereferenceable(256) %[[VAL_1:.*]], i8* noalias align 64 dereferenceable(256) %[[VAL_2:.*]]) {
// CHECK-LABEL: entry:
// CHECK:         %[[VAL_32:.*]] = getelementptr inbounds i8, i8* %[[VAL_33:.*]], i64 0
// CHECK:         %[[VAL_34:.*]] = bitcast i8* %[[VAL_32]] to [2 x [32 x [32 x float]]]*
// CHECK:         %[[VAL_35:.*]] = getelementptr inbounds i8, i8* %[[VAL_36:.*]], i64 0
// CHECK:         %[[VAL_37:.*]] = bitcast i8* %[[VAL_35]] to float*
// CHECK:         %[[VAL_38:.*]] = getelementptr inbounds i8, i8* %[[VAL_39:.*]], i64 0
// CHECK:         %[[VAL_40:.*]] = bitcast i8* %[[VAL_38]] to float*
// CHECK:         %[[VAL_41:.*]] = getelementptr inbounds i8, i8* %[[VAL_42:.*]], i64 0
// CHECK:         %[[VAL_43:.*]] = bitcast i8* %[[VAL_41]] to [2 x [32 x float]]*
// CHECK:         %[[VAL_44:.*]] = getelementptr inbounds i8, i8* %[[VAL_45:.*]], i64 0
// CHECK:         %[[VAL_46:.*]] = bitcast i8* %[[VAL_44]] to [2 x [32 x float]]*
// CHECK:         %[[VAL_47:.*]] = call i32 @llvm.nvvm.read.ptx.sreg.ctaid.x(), !range !6
// CHECK:         %[[VAL_48:.*]] = zext i32 %[[VAL_47]] to i64
// CHECK:         %[[VAL_49:.*]] = call i32 @llvm.nvvm.read.ptx.sreg.tid.x(), !range !7
// CHECK:         %[[VAL_50:.*]] = zext i32 %[[VAL_49]] to i64
// CHECK:         %[[VAL_51:.*]] = mul nuw nsw i64 %[[VAL_48]], 64
// CHECK:         %[[VAL_52:.*]] = add nuw nsw i64 %[[VAL_51]], %[[VAL_50]]
// CHECK:         %[[VAL_53:.*]] = icmp ult i64 %[[VAL_52]], 64
// CHECK:         call void @llvm.assume(i1 %[[VAL_53]])
// CHECK:         %[[VAL_54:.*]] = udiv i64 %[[VAL_52]], 1
// CHECK:         %[[VAL_55:.*]] = urem i64 %[[VAL_54]], 32
// CHECK:         %[[VAL_56:.*]] = udiv i64 %[[VAL_52]], 32
// CHECK:         %[[VAL_57:.*]] = icmp ult i64 %[[VAL_52]], 64
// CHECK:         br i1 %[[VAL_57]], label %[[VAL_58:.*]], label %[[VAL_59:.*]]
// CHECK:       fusion.in_bounds-after:                           ; preds = %[[VAL_58]], %[[VAL_60:.*]]
// CHECK:         ret void
// CHECK:       fusion.in_bounds-true:                            ; preds = %[[VAL_60]]
// CHECK:         %[[VAL_61:.*]] = load float, float* %[[VAL_40]], align 4, !invariant.load !8
// CHECK:         %[[VAL_62:.*]] = bitcast [2 x [32 x float]]* %[[VAL_46]] to float*
// CHECK:         %[[VAL_63:.*]] = getelementptr inbounds float, float* %[[VAL_62]], i64 %[[VAL_52]]
// CHECK:         store float %[[VAL_61]], float* %[[VAL_63]], align 4
// CHECK:         br label %[[VAL_59]]

// CHECK: define void @fusion__2(i8* noalias align 16 dereferenceable(8192) %[[VAL_0:.*]], i8* noalias align 16 dereferenceable(4) %[[VAL_3:.*]], i8* noalias align 16 dereferenceable(4) %[[VAL_4:.*]], i8* noalias align 64 dereferenceable(256) %[[VAL_1:.*]], i8* noalias align 64 dereferenceable(256) %[[VAL_2:.*]]) {
// CHECK-LABEL: entry:
// CHECK:         %[[VAL_64:.*]] = alloca i32, align 4
// CHECK:         %[[VAL_65:.*]] = alloca i32, align 4
// CHECK:         %[[VAL_66:.*]] = alloca float, align 4
// CHECK:         %[[VAL_67:.*]] = alloca float, align 4
// CHECK:         %[[VAL_68:.*]] = alloca float, align 4
// CHECK:         %[[VAL_69:.*]] = alloca float, align 4
// CHECK:         %[[VAL_70:.*]] = alloca float, align 4
// CHECK:         %[[VAL_71:.*]] = alloca float, align 4
// CHECK:         %[[VAL_72:.*]] = alloca float, align 4
// CHECK:         %[[VAL_73:.*]] = alloca float, align 4
// CHECK:         %[[VAL_74:.*]] = alloca float, align 4
// CHECK:         %[[VAL_75:.*]] = alloca float, align 4
// CHECK:         %[[VAL_76:.*]] = alloca float, align 4
// CHECK:         %[[VAL_77:.*]] = alloca float, align 4
// CHECK:         %[[VAL_78:.*]] = alloca float, align 4
// CHECK:         %[[VAL_79:.*]] = alloca float, align 4
// CHECK:         %[[VAL_80:.*]] = alloca float, align 4
// CHECK:         %[[VAL_81:.*]] = alloca float, align 4
// CHECK:         %[[VAL_82:.*]] = alloca float, align 4
// CHECK:         %[[VAL_83:.*]] = alloca float, align 4
// CHECK:         %[[VAL_84:.*]] = alloca float, align 4
// CHECK:         %[[VAL_85:.*]] = alloca float, align 4
// CHECK:         %[[VAL_86:.*]] = alloca float, align 4
// CHECK:         %[[VAL_87:.*]] = alloca float, align 4
// CHECK:         %[[VAL_88:.*]] = alloca i32, align 4
// CHECK:         %[[VAL_89:.*]] = alloca float, align 4
// CHECK:         %[[VAL_90:.*]] = alloca float, align 4
// CHECK:         %[[VAL_91:.*]] = alloca float, align 4
// CHECK:         %[[VAL_92:.*]] = alloca float, align 4
// CHECK:         %[[VAL_93:.*]] = getelementptr inbounds i8, i8* %[[VAL_94:.*]], i64 0
// CHECK:         %[[VAL_95:.*]] = bitcast i8* %[[VAL_93]] to [2 x [32 x [32 x float]]]*
// CHECK:         %[[VAL_96:.*]] = getelementptr inbounds i8, i8* %[[VAL_97:.*]], i64 0
// CHECK:         %[[VAL_98:.*]] = bitcast i8* %[[VAL_96]] to float*
// CHECK:         %[[VAL_99:.*]] = getelementptr inbounds i8, i8* %[[VAL_100:.*]], i64 0
// CHECK:         %[[VAL_101:.*]] = bitcast i8* %[[VAL_99]] to float*
// CHECK:         %[[VAL_102:.*]] = getelementptr inbounds i8, i8* %[[VAL_103:.*]], i64 0
// CHECK:         %[[VAL_104:.*]] = bitcast i8* %[[VAL_102]] to [2 x [32 x float]]*
// CHECK:         %[[VAL_105:.*]] = getelementptr inbounds i8, i8* %[[VAL_106:.*]], i64 0
// CHECK:         %[[VAL_107:.*]] = bitcast i8* %[[VAL_105]] to [2 x [32 x float]]*
// CHECK:         %[[VAL_108:.*]] = call i32 @llvm.nvvm.read.ptx.sreg.ctaid.y(), !range !6
// CHECK:         %[[VAL_109:.*]] = icmp eq i32 %[[VAL_108]], 0
// CHECK:         br i1 %[[VAL_109]], label %[[VAL_110:.*]], label %[[VAL_111:.*]]
// CHECK:       reduce-group-0-after:                             ; preds = %[[VAL_112:.*]], %[[VAL_113:.*]]
// CHECK:         ret void
// CHECK:       reduce-group-0-true:                              ; preds = %[[VAL_113]]
// CHECK:         %[[VAL_114:.*]] = load float, float* %[[VAL_98]], align 4, !invariant.load !8
// CHECK:         %[[VAL_115:.*]] = getelementptr inbounds float, float* %[[VAL_91]], i32 0
// CHECK:         store float %[[VAL_114]], float* %[[VAL_115]], align 4
// CHECK:         %[[VAL_116:.*]] = load float, float* %[[VAL_101]], align 4, !invariant.load !8
// CHECK:         %[[VAL_117:.*]] = getelementptr inbounds float, float* %[[VAL_89]], i32 0
// CHECK:         store float %[[VAL_116]], float* %[[VAL_117]], align 4
// CHECK:         %[[VAL_118:.*]] = call i32 @llvm.nvvm.read.ptx.sreg.tid.x(), !range !9
// CHECK:         %[[VAL_119:.*]] = urem i32 %[[VAL_118]], 32
// CHECK:         %[[VAL_120:.*]] = udiv i32 %[[VAL_118]], 32
// CHECK:         %[[VAL_121:.*]] = urem i32 %[[VAL_118]], 32
// CHECK:         %[[VAL_122:.*]] = call i32 @llvm.nvvm.read.ptx.sreg.ctaid.x(), !range !7
// CHECK:         %[[VAL_123:.*]] = udiv i32 %[[VAL_122]], 1
// CHECK:         %[[VAL_124:.*]] = urem i32 %[[VAL_123]], 1
// CHECK:         %[[VAL_125:.*]] = udiv i32 %[[VAL_122]], 1
// CHECK:         %[[VAL_126:.*]] = urem i32 %[[VAL_125]], 64
// CHECK:         %[[VAL_127:.*]] = udiv i32 %[[VAL_122]], 64
// CHECK:         %[[VAL_128:.*]] = mul i32 %[[VAL_127]], 1
// CHECK:         %[[VAL_129:.*]] = icmp eq i32 %[[VAL_126]], 63
// CHECK:         %[[VAL_130:.*]] = select i1 %[[VAL_129]], i32 1, i32 1
// CHECK:         %[[VAL_131:.*]] = icmp eq i32 %[[VAL_124]], 0
// CHECK:         %[[VAL_132:.*]] = select i1 %[[VAL_131]], i32 32, i32 256
// CHECK:         %[[VAL_133:.*]] = mul i32 %[[VAL_126]], 1
// CHECK:         %[[VAL_134:.*]] = mul i32 %[[VAL_124]], 256
// CHECK:         %[[VAL_135:.*]] = mul i32 %[[VAL_119]], 2
// CHECK:         %[[VAL_136:.*]] = add i32 %[[VAL_134]], %[[VAL_135]]
// CHECK:         %[[VAL_137:.*]] = sub i32 %[[VAL_130]], %[[VAL_120]]
// CHECK:         %[[VAL_138:.*]] = add i32 %[[VAL_137]], 1
// CHECK:         %[[VAL_139:.*]] = add i32 %[[VAL_138]], -1
// CHECK:         %[[VAL_140:.*]] = udiv i32 %[[VAL_139]], 1
// CHECK:         store i32 0, i32* %[[VAL_88]], align 4
// CHECK:         br label %[[VAL_141:.*]]
// CHECK:       output_y_in_tile.loop_header:                     ; preds = %[[VAL_142:.*]], %[[VAL_110]]
// CHECK:         %[[VAL_143:.*]] = load i32, i32* %[[VAL_88]], align 4
// CHECK:         %[[VAL_144:.*]] = icmp uge i32 %[[VAL_143]], %[[VAL_140]]
// CHECK:         br i1 %[[VAL_144]], label %[[VAL_145:.*]], label %[[VAL_146:.*]]
// CHECK:       output_y_in_tile.loop_body:                       ; preds = %[[VAL_141]]
// CHECK:         %[[VAL_147:.*]] = add nuw nsw i32 %[[VAL_143]], 1
// CHECK:         store i32 %[[VAL_147]], i32* %[[VAL_88]], align 4
// CHECK:         %[[VAL_148:.*]] = icmp eq i32 %[[VAL_143]], 0
// CHECK:         %[[VAL_149:.*]] = mul i32 %[[VAL_143]], 1
// CHECK:         %[[VAL_150:.*]] = add i32 %[[VAL_120]], %[[VAL_149]]
// CHECK:         %[[VAL_151:.*]] = icmp eq i32 256, %[[VAL_132]]
// CHECK:         br i1 %[[VAL_151]], label %[[VAL_152:.*]], label %[[VAL_153:.*]]
// CHECK:       output_is_full_tile-after:                        ; preds = %[[VAL_154:.*]], %[[VAL_152]]
// CHECK:         br label %[[VAL_141]], !llvm.loop !10
// CHECK:       output_y_in_tile.loop_exit:                       ; preds = %[[VAL_141]]
// CHECK:         %[[VAL_155:.*]] = call i32 @llvm.nvvm.read.ptx.sreg.tid.x(), !range !9
// CHECK:         %[[VAL_156:.*]] = urem i32 %[[VAL_155]], 32
// CHECK:         %[[VAL_157:.*]] = udiv i32 %[[VAL_155]], 32
// CHECK:         %[[VAL_158:.*]] = urem i32 %[[VAL_155]], 32
// CHECK:         %[[VAL_159:.*]] = mul i32 %[[VAL_156]], 2
// CHECK:         %[[VAL_160:.*]] = add i32 %[[VAL_133]], %[[VAL_157]]
// CHECK:         %[[VAL_161:.*]] = add i32 %[[VAL_134]], %[[VAL_159]]
// CHECK:         %[[VAL_162:.*]] = add i32 %[[VAL_161]], 0
// CHECK:         %[[VAL_163:.*]] = udiv i32 %[[VAL_160]], 1
// CHECK:         %[[VAL_164:.*]] = urem i32 %[[VAL_163]], 32
// CHECK:         %[[VAL_165:.*]] = udiv i32 %[[VAL_160]], 32
// CHECK:         %[[VAL_166:.*]] = getelementptr inbounds [2 x [32 x float]], [2 x [32 x float]]* %[[VAL_104]], i32 0, i32 %[[VAL_165]], i32 %[[VAL_164]]
// CHECK:         %[[VAL_167:.*]] = getelementptr inbounds float, float* %[[VAL_91]], i32 0
// CHECK:         %[[VAL_168:.*]] = load float, float* %[[VAL_167]], align 4
// CHECK:         %[[VAL_169:.*]] = call float @llvm.nvvm.shfl.sync.down.f32(i32 -1, float %[[VAL_168]], i32 16, i32 31)
// CHECK:         store float %[[VAL_169]], float* %[[VAL_87]], align 4
// CHECK:         call void @region_1_4(float* %[[VAL_167]], float* %[[VAL_87]], float* %[[VAL_167]])
// CHECK:         %[[VAL_170:.*]] = load float, float* %[[VAL_167]], align 4
// CHECK:         %[[VAL_171:.*]] = call float @llvm.nvvm.shfl.sync.down.f32(i32 -1, float %[[VAL_170]], i32 8, i32 31)
// CHECK:         store float %[[VAL_171]], float* %[[VAL_86]], align 4
// CHECK:         call void @region_1_4(float* %[[VAL_167]], float* %[[VAL_86]], float* %[[VAL_167]])
// CHECK:         %[[VAL_172:.*]] = load float, float* %[[VAL_167]], align 4
// CHECK:         %[[VAL_173:.*]] = call float @llvm.nvvm.shfl.sync.down.f32(i32 -1, float %[[VAL_172]], i32 4, i32 31)
// CHECK:         store float %[[VAL_173]], float* %[[VAL_85]], align 4
// CHECK:         call void @region_1_4(float* %[[VAL_167]], float* %[[VAL_85]], float* %[[VAL_167]])
// CHECK:         %[[VAL_174:.*]] = load float, float* %[[VAL_167]], align 4
// CHECK:         %[[VAL_175:.*]] = call float @llvm.nvvm.shfl.sync.down.f32(i32 -1, float %[[VAL_174]], i32 2, i32 31)
// CHECK:         store float %[[VAL_175]], float* %[[VAL_84]], align 4
// CHECK:         call void @region_1_4(float* %[[VAL_167]], float* %[[VAL_84]], float* %[[VAL_167]])
// CHECK:         %[[VAL_176:.*]] = load float, float* %[[VAL_167]], align 4
// CHECK:         %[[VAL_177:.*]] = call float @llvm.nvvm.shfl.sync.down.f32(i32 -1, float %[[VAL_176]], i32 1, i32 31)
// CHECK:         store float %[[VAL_177]], float* %[[VAL_83]], align 4
// CHECK:         call void @region_1_4(float* %[[VAL_167]], float* %[[VAL_83]], float* %[[VAL_167]])
// CHECK:         %[[VAL_178:.*]] = udiv i32 %[[VAL_156]], 32
// CHECK:         %[[VAL_179:.*]] = icmp eq i32 %[[VAL_158]], 0
// CHECK:         br i1 %[[VAL_179]], label %[[VAL_180:.*]], label %[[VAL_181:.*]]
// CHECK:       intra_warp_reduce_write-after:                    ; preds = %[[VAL_180]], %[[VAL_145]]
// CHECK:         call void @llvm.nvvm.barrier0()
// CHECK:         %[[VAL_182:.*]] = icmp eq i32 %[[VAL_178]], 0
// CHECK:         br i1 %[[VAL_182]], label %[[VAL_183:.*]], label %[[VAL_184:.*]]
// CHECK:       inter_warp_reduce-after:                          ; preds = %[[VAL_185:.*]], %[[VAL_181]]
// CHECK:         %[[VAL_186:.*]] = add i32 %[[VAL_161]], 0
// CHECK:         %[[VAL_187:.*]] = udiv i32 %[[VAL_160]], 1
// CHECK:         %[[VAL_188:.*]] = urem i32 %[[VAL_187]], 32
// CHECK:         %[[VAL_189:.*]] = udiv i32 %[[VAL_160]], 32
// CHECK:         %[[VAL_190:.*]] = getelementptr inbounds [2 x [32 x float]], [2 x [32 x float]]* %[[VAL_107]], i32 0, i32 %[[VAL_189]], i32 %[[VAL_188]]
// CHECK:         %[[VAL_191:.*]] = getelementptr inbounds float, float* %[[VAL_89]], i32 0
// CHECK:         %[[VAL_192:.*]] = load float, float* %[[VAL_191]], align 4
// CHECK:         %[[VAL_193:.*]] = call float @llvm.nvvm.shfl.sync.down.f32(i32 -1, float %[[VAL_192]], i32 16, i32 31)
// CHECK:         store float %[[VAL_193]], float* %[[VAL_76]], align 4
// CHECK:         call void @region_2_9(float* %[[VAL_191]], float* %[[VAL_76]], float* %[[VAL_191]])
// CHECK:         %[[VAL_194:.*]] = load float, float* %[[VAL_191]], align 4
// CHECK:         %[[VAL_195:.*]] = call float @llvm.nvvm.shfl.sync.down.f32(i32 -1, float %[[VAL_194]], i32 8, i32 31)
// CHECK:         store float %[[VAL_195]], float* %[[VAL_75]], align 4
// CHECK:         call void @region_2_9(float* %[[VAL_191]], float* %[[VAL_75]], float* %[[VAL_191]])
// CHECK:         %[[VAL_196:.*]] = load float, float* %[[VAL_191]], align 4
// CHECK:         %[[VAL_197:.*]] = call float @llvm.nvvm.shfl.sync.down.f32(i32 -1, float %[[VAL_196]], i32 4, i32 31)
// CHECK:         store float %[[VAL_197]], float* %[[VAL_74]], align 4
// CHECK:         call void @region_2_9(float* %[[VAL_191]], float* %[[VAL_74]], float* %[[VAL_191]])
// CHECK:         %[[VAL_198:.*]] = load float, float* %[[VAL_191]], align 4
// CHECK:         %[[VAL_199:.*]] = call float @llvm.nvvm.shfl.sync.down.f32(i32 -1, float %[[VAL_198]], i32 2, i32 31)
// CHECK:         store float %[[VAL_199]], float* %[[VAL_73]], align 4
// CHECK:         call void @region_2_9(float* %[[VAL_191]], float* %[[VAL_73]], float* %[[VAL_191]])
// CHECK:         %[[VAL_200:.*]] = load float, float* %[[VAL_191]], align 4
// CHECK:         %[[VAL_201:.*]] = call float @llvm.nvvm.shfl.sync.down.f32(i32 -1, float %[[VAL_200]], i32 1, i32 31)
// CHECK:         store float %[[VAL_201]], float* %[[VAL_72]], align 4
// CHECK:         call void @region_2_9(float* %[[VAL_191]], float* %[[VAL_72]], float* %[[VAL_191]])
// CHECK:         %[[VAL_202:.*]] = udiv i32 %[[VAL_156]], 32
// CHECK:         %[[VAL_203:.*]] = icmp eq i32 %[[VAL_158]], 0
// CHECK:         br i1 %[[VAL_203]], label %[[VAL_204:.*]], label %[[VAL_205:.*]]
// CHECK:       intra_warp_reduce_write-after129:                 ; preds = %[[VAL_204]], %[[VAL_184]]
// CHECK:         call void @llvm.nvvm.barrier0()
// CHECK:         %[[VAL_206:.*]] = icmp eq i32 %[[VAL_202]], 0
// CHECK:         br i1 %[[VAL_206]], label %[[VAL_207:.*]], label %[[VAL_112]]
// CHECK:       inter_warp_reduce-after131:                       ; preds = %[[VAL_208:.*]], %[[VAL_205]]
// CHECK:         br label %[[VAL_111]]
// CHECK:       output_is_full_tile-true:                         ; preds = %[[VAL_146]]
// CHECK:         %[[VAL_209:.*]] = add i32 %[[VAL_133]], %[[VAL_150]]
// CHECK:         %[[VAL_210:.*]] = add i32 0, %[[VAL_135]]
// CHECK:         %[[VAL_211:.*]] = add i32 %[[VAL_136]], 0
// CHECK:         %[[VAL_212:.*]] = mul nuw nsw i32 %[[VAL_211]], 1
// CHECK:         %[[VAL_213:.*]] = add nuw nsw i32 0, %[[VAL_212]]
// CHECK:         %[[VAL_214:.*]] = mul nuw nsw i32 %[[VAL_209]], 32
// CHECK:         %[[VAL_215:.*]] = add nuw nsw i32 %[[VAL_213]], %[[VAL_214]]
// CHECK:         %[[VAL_216:.*]] = mul nuw nsw i32 %[[VAL_128]], 2048
// CHECK:         %[[VAL_217:.*]] = add nuw nsw i32 %[[VAL_215]], %[[VAL_216]]
// CHECK:         %[[VAL_218:.*]] = udiv i32 %[[VAL_217]], 1
// CHECK:         %[[VAL_219:.*]] = urem i32 %[[VAL_218]], 32
// CHECK:         %[[VAL_220:.*]] = udiv i32 %[[VAL_217]], 32
// CHECK:         %[[VAL_221:.*]] = urem i32 %[[VAL_220]], 32
// CHECK:         %[[VAL_222:.*]] = udiv i32 %[[VAL_217]], 1024
// CHECK:         %[[VAL_223:.*]] = bitcast [2 x [32 x [32 x float]]]* %[[VAL_95]] to float*
// CHECK:         %[[VAL_224:.*]] = getelementptr inbounds float, float* %[[VAL_223]], i32 %[[VAL_217]]
// CHECK:         %[[VAL_225:.*]] = load float, float* %[[VAL_224]], align 4, !invariant.load !8
// CHECK:         store float %[[VAL_225]], float* %[[VAL_92]], align 4
// CHECK:         %[[VAL_226:.*]] = getelementptr inbounds float, float* %[[VAL_91]], i32 0
// CHECK:         call void @region_1_4(float* %[[VAL_226]], float* %[[VAL_92]], float* %[[VAL_226]])
// CHECK:         %[[VAL_227:.*]] = bitcast [2 x [32 x [32 x float]]]* %[[VAL_95]] to float*
// CHECK:         %[[VAL_228:.*]] = getelementptr inbounds float, float* %[[VAL_227]], i32 %[[VAL_217]]
// CHECK:         %[[VAL_229:.*]] = load float, float* %[[VAL_228]], align 4, !invariant.load !8
// CHECK:         store float %[[VAL_229]], float* %[[VAL_90]], align 4
// CHECK:         %[[VAL_230:.*]] = getelementptr inbounds float, float* %[[VAL_89]], i32 0
// CHECK:         call void @region_2_9(float* %[[VAL_230]], float* %[[VAL_90]], float* %[[VAL_230]])
// CHECK:         %[[VAL_231:.*]] = add i32 1, %[[VAL_135]]
// CHECK:         %[[VAL_232:.*]] = add i32 %[[VAL_136]], 1
// CHECK:         %[[VAL_233:.*]] = mul nuw nsw i32 %[[VAL_232]], 1
// CHECK:         %[[VAL_234:.*]] = add nuw nsw i32 0, %[[VAL_233]]
// CHECK:         %[[VAL_235:.*]] = mul nuw nsw i32 %[[VAL_209]], 32
// CHECK:         %[[VAL_236:.*]] = add nuw nsw i32 %[[VAL_234]], %[[VAL_235]]
// CHECK:         %[[VAL_237:.*]] = mul nuw nsw i32 %[[VAL_128]], 2048
// CHECK:         %[[VAL_238:.*]] = add nuw nsw i32 %[[VAL_236]], %[[VAL_237]]
// CHECK:         %[[VAL_239:.*]] = udiv i32 %[[VAL_238]], 1
// CHECK:         %[[VAL_240:.*]] = urem i32 %[[VAL_239]], 32
// CHECK:         %[[VAL_241:.*]] = udiv i32 %[[VAL_238]], 32
// CHECK:         %[[VAL_242:.*]] = urem i32 %[[VAL_241]], 32
// CHECK:         %[[VAL_243:.*]] = udiv i32 %[[VAL_238]], 1024
// CHECK:         %[[VAL_244:.*]] = bitcast [2 x [32 x [32 x float]]]* %[[VAL_95]] to float*
// CHECK:         %[[VAL_245:.*]] = getelementptr inbounds float, float* %[[VAL_244]], i32 %[[VAL_238]]
// CHECK:         %[[VAL_246:.*]] = load float, float* %[[VAL_245]], align 4, !invariant.load !8
// CHECK:         store float %[[VAL_246]], float* %[[VAL_92]], align 4
// CHECK:         %[[VAL_247:.*]] = getelementptr inbounds float, float* %[[VAL_91]], i32 0
// CHECK:         call void @region_1_4(float* %[[VAL_247]], float* %[[VAL_92]], float* %[[VAL_247]])
// CHECK:         %[[VAL_248:.*]] = bitcast [2 x [32 x [32 x float]]]* %[[VAL_95]] to float*
// CHECK:         %[[VAL_249:.*]] = getelementptr inbounds float, float* %[[VAL_248]], i32 %[[VAL_238]]
// CHECK:         %[[VAL_250:.*]] = load float, float* %[[VAL_249]], align 4, !invariant.load !8
// CHECK:         store float %[[VAL_250]], float* %[[VAL_90]], align 4
// CHECK:         %[[VAL_251:.*]] = getelementptr inbounds float, float* %[[VAL_89]], i32 0
// CHECK:         call void @region_2_9(float* %[[VAL_251]], float* %[[VAL_90]], float* %[[VAL_251]])
// CHECK:         %[[VAL_252:.*]] = add i32 64, %[[VAL_135]]
// CHECK:         %[[VAL_253:.*]] = add i32 %[[VAL_136]], 64
// CHECK:         %[[VAL_254:.*]] = mul nuw nsw i32 %[[VAL_253]], 1
// CHECK:         %[[VAL_255:.*]] = add nuw nsw i32 0, %[[VAL_254]]
// CHECK:         %[[VAL_256:.*]] = mul nuw nsw i32 %[[VAL_209]], 32
// CHECK:         %[[VAL_257:.*]] = add nuw nsw i32 %[[VAL_255]], %[[VAL_256]]
// CHECK:         %[[VAL_258:.*]] = mul nuw nsw i32 %[[VAL_128]], 2048
// CHECK:         %[[VAL_259:.*]] = add nuw nsw i32 %[[VAL_257]], %[[VAL_258]]
// CHECK:         %[[VAL_260:.*]] = udiv i32 %[[VAL_259]], 1
// CHECK:         %[[VAL_261:.*]] = urem i32 %[[VAL_260]], 32
// CHECK:         %[[VAL_262:.*]] = udiv i32 %[[VAL_259]], 32
// CHECK:         %[[VAL_263:.*]] = urem i32 %[[VAL_262]], 32
// CHECK:         %[[VAL_264:.*]] = udiv i32 %[[VAL_259]], 1024
// CHECK:         %[[VAL_265:.*]] = bitcast [2 x [32 x [32 x float]]]* %[[VAL_95]] to float*
// CHECK:         %[[VAL_266:.*]] = getelementptr inbounds float, float* %[[VAL_265]], i32 %[[VAL_259]]
// CHECK:         %[[VAL_267:.*]] = load float, float* %[[VAL_266]], align 4, !invariant.load !8
// CHECK:         store float %[[VAL_267]], float* %[[VAL_92]], align 4
// CHECK:         %[[VAL_268:.*]] = getelementptr inbounds float, float* %[[VAL_91]], i32 0
// CHECK:         call void @region_1_4(float* %[[VAL_268]], float* %[[VAL_92]], float* %[[VAL_268]])
// CHECK:         %[[VAL_269:.*]] = bitcast [2 x [32 x [32 x float]]]* %[[VAL_95]] to float*
// CHECK:         %[[VAL_270:.*]] = getelementptr inbounds float, float* %[[VAL_269]], i32 %[[VAL_259]]
// CHECK:         %[[VAL_271:.*]] = load float, float* %[[VAL_270]], align 4, !invariant.load !8
// CHECK:         store float %[[VAL_271]], float* %[[VAL_90]], align 4
// CHECK:         %[[VAL_272:.*]] = getelementptr inbounds float, float* %[[VAL_89]], i32 0
// CHECK:         call void @region_2_9(float* %[[VAL_272]], float* %[[VAL_90]], float* %[[VAL_272]])
// CHECK:         %[[VAL_273:.*]] = add i32 65, %[[VAL_135]]
// CHECK:         %[[VAL_274:.*]] = add i32 %[[VAL_136]], 65
// CHECK:         %[[VAL_275:.*]] = mul nuw nsw i32 %[[VAL_274]], 1
// CHECK:         %[[VAL_276:.*]] = add nuw nsw i32 0, %[[VAL_275]]
// CHECK:         %[[VAL_277:.*]] = mul nuw nsw i32 %[[VAL_209]], 32
// CHECK:         %[[VAL_278:.*]] = add nuw nsw i32 %[[VAL_276]], %[[VAL_277]]
// CHECK:         %[[VAL_279:.*]] = mul nuw nsw i32 %[[VAL_128]], 2048
// CHECK:         %[[VAL_280:.*]] = add nuw nsw i32 %[[VAL_278]], %[[VAL_279]]
// CHECK:         %[[VAL_281:.*]] = udiv i32 %[[VAL_280]], 1
// CHECK:         %[[VAL_282:.*]] = urem i32 %[[VAL_281]], 32
// CHECK:         %[[VAL_283:.*]] = udiv i32 %[[VAL_280]], 32
// CHECK:         %[[VAL_284:.*]] = urem i32 %[[VAL_283]], 32
// CHECK:         %[[VAL_285:.*]] = udiv i32 %[[VAL_280]], 1024
// CHECK:         %[[VAL_286:.*]] = bitcast [2 x [32 x [32 x float]]]* %[[VAL_95]] to float*
// CHECK:         %[[VAL_287:.*]] = getelementptr inbounds float, float* %[[VAL_286]], i32 %[[VAL_280]]
// CHECK:         %[[VAL_288:.*]] = load float, float* %[[VAL_287]], align 4, !invariant.load !8
// CHECK:         store float %[[VAL_288]], float* %[[VAL_92]], align 4
// CHECK:         %[[VAL_289:.*]] = getelementptr inbounds float, float* %[[VAL_91]], i32 0
// CHECK:         call void @region_1_4(float* %[[VAL_289]], float* %[[VAL_92]], float* %[[VAL_289]])
// CHECK:         %[[VAL_290:.*]] = bitcast [2 x [32 x [32 x float]]]* %[[VAL_95]] to float*
// CHECK:         %[[VAL_291:.*]] = getelementptr inbounds float, float* %[[VAL_290]], i32 %[[VAL_280]]
// CHECK:         %[[VAL_292:.*]] = load float, float* %[[VAL_291]], align 4, !invariant.load !8
// CHECK:         store float %[[VAL_292]], float* %[[VAL_90]], align 4
// CHECK:         %[[VAL_293:.*]] = getelementptr inbounds float, float* %[[VAL_89]], i32 0
// CHECK:         call void @region_2_9(float* %[[VAL_293]], float* %[[VAL_90]], float* %[[VAL_293]])
// CHECK:         %[[VAL_294:.*]] = add i32 128, %[[VAL_135]]
// CHECK:         %[[VAL_295:.*]] = add i32 %[[VAL_136]], 128
// CHECK:         %[[VAL_296:.*]] = mul nuw nsw i32 %[[VAL_295]], 1
// CHECK:         %[[VAL_297:.*]] = add nuw nsw i32 0, %[[VAL_296]]
// CHECK:         %[[VAL_298:.*]] = mul nuw nsw i32 %[[VAL_209]], 32
// CHECK:         %[[VAL_299:.*]] = add nuw nsw i32 %[[VAL_297]], %[[VAL_298]]
// CHECK:         %[[VAL_300:.*]] = mul nuw nsw i32 %[[VAL_128]], 2048
// CHECK:         %[[VAL_301:.*]] = add nuw nsw i32 %[[VAL_299]], %[[VAL_300]]
// CHECK:         %[[VAL_302:.*]] = udiv i32 %[[VAL_301]], 1
// CHECK:         %[[VAL_303:.*]] = urem i32 %[[VAL_302]], 32
// CHECK:         %[[VAL_304:.*]] = udiv i32 %[[VAL_301]], 32
// CHECK:         %[[VAL_305:.*]] = urem i32 %[[VAL_304]], 32
// CHECK:         %[[VAL_306:.*]] = udiv i32 %[[VAL_301]], 1024
// CHECK:         %[[VAL_307:.*]] = bitcast [2 x [32 x [32 x float]]]* %[[VAL_95]] to float*
// CHECK:         %[[VAL_308:.*]] = getelementptr inbounds float, float* %[[VAL_307]], i32 %[[VAL_301]]
// CHECK:         %[[VAL_309:.*]] = load float, float* %[[VAL_308]], align 4, !invariant.load !8
// CHECK:         store float %[[VAL_309]], float* %[[VAL_92]], align 4
// CHECK:         %[[VAL_310:.*]] = getelementptr inbounds float, float* %[[VAL_91]], i32 0
// CHECK:         call void @region_1_4(float* %[[VAL_310]], float* %[[VAL_92]], float* %[[VAL_310]])
// CHECK:         %[[VAL_311:.*]] = bitcast [2 x [32 x [32 x float]]]* %[[VAL_95]] to float*
// CHECK:         %[[VAL_312:.*]] = getelementptr inbounds float, float* %[[VAL_311]], i32 %[[VAL_301]]
// CHECK:         %[[VAL_313:.*]] = load float, float* %[[VAL_312]], align 4, !invariant.load !8
// CHECK:         store float %[[VAL_313]], float* %[[VAL_90]], align 4
// CHECK:         %[[VAL_314:.*]] = getelementptr inbounds float, float* %[[VAL_89]], i32 0
// CHECK:         call void @region_2_9(float* %[[VAL_314]], float* %[[VAL_90]], float* %[[VAL_314]])
// CHECK:         %[[VAL_315:.*]] = add i32 129, %[[VAL_135]]
// CHECK:         %[[VAL_316:.*]] = add i32 %[[VAL_136]], 129
// CHECK:         %[[VAL_317:.*]] = mul nuw nsw i32 %[[VAL_316]], 1
// CHECK:         %[[VAL_318:.*]] = add nuw nsw i32 0, %[[VAL_317]]
// CHECK:         %[[VAL_319:.*]] = mul nuw nsw i32 %[[VAL_209]], 32
// CHECK:         %[[VAL_320:.*]] = add nuw nsw i32 %[[VAL_318]], %[[VAL_319]]
// CHECK:         %[[VAL_321:.*]] = mul nuw nsw i32 %[[VAL_128]], 2048
// CHECK:         %[[VAL_322:.*]] = add nuw nsw i32 %[[VAL_320]], %[[VAL_321]]
// CHECK:         %[[VAL_323:.*]] = udiv i32 %[[VAL_322]], 1
// CHECK:         %[[VAL_324:.*]] = urem i32 %[[VAL_323]], 32
// CHECK:         %[[VAL_325:.*]] = udiv i32 %[[VAL_322]], 32
// CHECK:         %[[VAL_326:.*]] = urem i32 %[[VAL_325]], 32
// CHECK:         %[[VAL_327:.*]] = udiv i32 %[[VAL_322]], 1024
// CHECK:         %[[VAL_328:.*]] = bitcast [2 x [32 x [32 x float]]]* %[[VAL_95]] to float*
// CHECK:         %[[VAL_329:.*]] = getelementptr inbounds float, float* %[[VAL_328]], i32 %[[VAL_322]]
// CHECK:         %[[VAL_330:.*]] = load float, float* %[[VAL_329]], align 4, !invariant.load !8
// CHECK:         store float %[[VAL_330]], float* %[[VAL_92]], align 4
// CHECK:         %[[VAL_331:.*]] = getelementptr inbounds float, float* %[[VAL_91]], i32 0
// CHECK:         call void @region_1_4(float* %[[VAL_331]], float* %[[VAL_92]], float* %[[VAL_331]])
// CHECK:         %[[VAL_332:.*]] = bitcast [2 x [32 x [32 x float]]]* %[[VAL_95]] to float*
// CHECK:         %[[VAL_333:.*]] = getelementptr inbounds float, float* %[[VAL_332]], i32 %[[VAL_322]]
// CHECK:         %[[VAL_334:.*]] = load float, float* %[[VAL_333]], align 4, !invariant.load !8
// CHECK:         store float %[[VAL_334]], float* %[[VAL_90]], align 4
// CHECK:         %[[VAL_335:.*]] = getelementptr inbounds float, float* %[[VAL_89]], i32 0
// CHECK:         call void @region_2_9(float* %[[VAL_335]], float* %[[VAL_90]], float* %[[VAL_335]])
// CHECK:         %[[VAL_336:.*]] = add i32 192, %[[VAL_135]]
// CHECK:         %[[VAL_337:.*]] = add i32 %[[VAL_136]], 192
// CHECK:         %[[VAL_338:.*]] = mul nuw nsw i32 %[[VAL_337]], 1
// CHECK:         %[[VAL_339:.*]] = add nuw nsw i32 0, %[[VAL_338]]
// CHECK:         %[[VAL_340:.*]] = mul nuw nsw i32 %[[VAL_209]], 32
// CHECK:         %[[VAL_341:.*]] = add nuw nsw i32 %[[VAL_339]], %[[VAL_340]]
// CHECK:         %[[VAL_342:.*]] = mul nuw nsw i32 %[[VAL_128]], 2048
// CHECK:         %[[VAL_343:.*]] = add nuw nsw i32 %[[VAL_341]], %[[VAL_342]]
// CHECK:         %[[VAL_344:.*]] = udiv i32 %[[VAL_343]], 1
// CHECK:         %[[VAL_345:.*]] = urem i32 %[[VAL_344]], 32
// CHECK:         %[[VAL_346:.*]] = udiv i32 %[[VAL_343]], 32
// CHECK:         %[[VAL_347:.*]] = urem i32 %[[VAL_346]], 32
// CHECK:         %[[VAL_348:.*]] = udiv i32 %[[VAL_343]], 1024
// CHECK:         %[[VAL_349:.*]] = bitcast [2 x [32 x [32 x float]]]* %[[VAL_95]] to float*
// CHECK:         %[[VAL_350:.*]] = getelementptr inbounds float, float* %[[VAL_349]], i32 %[[VAL_343]]
// CHECK:         %[[VAL_351:.*]] = load float, float* %[[VAL_350]], align 4, !invariant.load !8
// CHECK:         store float %[[VAL_351]], float* %[[VAL_92]], align 4
// CHECK:         %[[VAL_352:.*]] = getelementptr inbounds float, float* %[[VAL_91]], i32 0
// CHECK:         call void @region_1_4(float* %[[VAL_352]], float* %[[VAL_92]], float* %[[VAL_352]])
// CHECK:         %[[VAL_353:.*]] = bitcast [2 x [32 x [32 x float]]]* %[[VAL_95]] to float*
// CHECK:         %[[VAL_354:.*]] = getelementptr inbounds float, float* %[[VAL_353]], i32 %[[VAL_343]]
// CHECK:         %[[VAL_355:.*]] = load float, float* %[[VAL_354]], align 4, !invariant.load !8
// CHECK:         store float %[[VAL_355]], float* %[[VAL_90]], align 4
// CHECK:         %[[VAL_356:.*]] = getelementptr inbounds float, float* %[[VAL_89]], i32 0
// CHECK:         call void @region_2_9(float* %[[VAL_356]], float* %[[VAL_90]], float* %[[VAL_356]])
// CHECK:         %[[VAL_357:.*]] = add i32 193, %[[VAL_135]]
// CHECK:         %[[VAL_358:.*]] = add i32 %[[VAL_136]], 193
// CHECK:         %[[VAL_359:.*]] = mul nuw nsw i32 %[[VAL_358]], 1
// CHECK:         %[[VAL_360:.*]] = add nuw nsw i32 0, %[[VAL_359]]
// CHECK:         %[[VAL_361:.*]] = mul nuw nsw i32 %[[VAL_209]], 32
// CHECK:         %[[VAL_362:.*]] = add nuw nsw i32 %[[VAL_360]], %[[VAL_361]]
// CHECK:         %[[VAL_363:.*]] = mul nuw nsw i32 %[[VAL_128]], 2048
// CHECK:         %[[VAL_364:.*]] = add nuw nsw i32 %[[VAL_362]], %[[VAL_363]]
// CHECK:         %[[VAL_365:.*]] = udiv i32 %[[VAL_364]], 1
// CHECK:         %[[VAL_366:.*]] = urem i32 %[[VAL_365]], 32
// CHECK:         %[[VAL_367:.*]] = udiv i32 %[[VAL_364]], 32
// CHECK:         %[[VAL_368:.*]] = urem i32 %[[VAL_367]], 32
// CHECK:         %[[VAL_369:.*]] = udiv i32 %[[VAL_364]], 1024
// CHECK:         %[[VAL_370:.*]] = bitcast [2 x [32 x [32 x float]]]* %[[VAL_95]] to float*
// CHECK:         %[[VAL_371:.*]] = getelementptr inbounds float, float* %[[VAL_370]], i32 %[[VAL_364]]
// CHECK:         %[[VAL_372:.*]] = load float, float* %[[VAL_371]], align 4, !invariant.load !8
// CHECK:         store float %[[VAL_372]], float* %[[VAL_92]], align 4
// CHECK:         %[[VAL_373:.*]] = getelementptr inbounds float, float* %[[VAL_91]], i32 0
// CHECK:         call void @region_1_4(float* %[[VAL_373]], float* %[[VAL_92]], float* %[[VAL_373]])
// CHECK:         %[[VAL_374:.*]] = bitcast [2 x [32 x [32 x float]]]* %[[VAL_95]] to float*
// CHECK:         %[[VAL_375:.*]] = getelementptr inbounds float, float* %[[VAL_374]], i32 %[[VAL_364]]
// CHECK:         %[[VAL_376:.*]] = load float, float* %[[VAL_375]], align 4, !invariant.load !8
// CHECK:         store float %[[VAL_376]], float* %[[VAL_90]], align 4
// CHECK:         %[[VAL_377:.*]] = getelementptr inbounds float, float* %[[VAL_89]], i32 0
// CHECK:         call void @region_2_9(float* %[[VAL_377]], float* %[[VAL_90]], float* %[[VAL_377]])
// CHECK:         br label %[[VAL_142]]
// CHECK:       output_is_full_tile-false:                        ; preds = %[[VAL_146]]
// CHECK:         %[[VAL_378:.*]] = add i32 %[[VAL_133]], %[[VAL_150]]
// CHECK:         %[[VAL_379:.*]] = add i32 0, %[[VAL_135]]
// CHECK:         %[[VAL_380:.*]] = add i32 %[[VAL_136]], 0
// CHECK:         %[[VAL_381:.*]] = icmp ult i32 %[[VAL_379]], %[[VAL_132]]
// CHECK:         br i1 %[[VAL_381]], label %[[VAL_382:.*]], label %[[VAL_383:.*]]
// CHECK:       output_x_in_tile-after:                           ; preds = %[[VAL_382]], %[[VAL_153]]
// CHECK:         %[[VAL_384:.*]] = add i32 1, %[[VAL_135]]
// CHECK:         %[[VAL_385:.*]] = add i32 %[[VAL_136]], 1
// CHECK:         %[[VAL_386:.*]] = icmp ult i32 %[[VAL_384]], %[[VAL_132]]
// CHECK:         br i1 %[[VAL_386]], label %[[VAL_387:.*]], label %[[VAL_388:.*]]
// CHECK:       output_x_in_tile-after48:                         ; preds = %[[VAL_387]], %[[VAL_383]]
// CHECK:         %[[VAL_389:.*]] = add i32 64, %[[VAL_135]]
// CHECK:         %[[VAL_390:.*]] = add i32 %[[VAL_136]], 64
// CHECK:         %[[VAL_391:.*]] = icmp ult i32 %[[VAL_389]], %[[VAL_132]]
// CHECK:         br i1 %[[VAL_391]], label %[[VAL_392:.*]], label %[[VAL_393:.*]]
// CHECK:       output_x_in_tile-after55:                         ; preds = %[[VAL_392]], %[[VAL_388]]
// CHECK:         %[[VAL_394:.*]] = add i32 65, %[[VAL_135]]
// CHECK:         %[[VAL_395:.*]] = add i32 %[[VAL_136]], 65
// CHECK:         %[[VAL_396:.*]] = icmp ult i32 %[[VAL_394]], %[[VAL_132]]
// CHECK:         br i1 %[[VAL_396]], label %[[VAL_397:.*]], label %[[VAL_398:.*]]
// CHECK:       output_x_in_tile-after62:                         ; preds = %[[VAL_397]], %[[VAL_393]]
// CHECK:         %[[VAL_399:.*]] = add i32 128, %[[VAL_135]]
// CHECK:         %[[VAL_400:.*]] = add i32 %[[VAL_136]], 128
// CHECK:         %[[VAL_401:.*]] = icmp ult i32 %[[VAL_399]], %[[VAL_132]]
// CHECK:         br i1 %[[VAL_401]], label %[[VAL_402:.*]], label %[[VAL_403:.*]]
// CHECK:       output_x_in_tile-after69:                         ; preds = %[[VAL_402]], %[[VAL_398]]
// CHECK:         %[[VAL_404:.*]] = add i32 129, %[[VAL_135]]
// CHECK:         %[[VAL_405:.*]] = add i32 %[[VAL_136]], 129
// CHECK:         %[[VAL_406:.*]] = icmp ult i32 %[[VAL_404]], %[[VAL_132]]
// CHECK:         br i1 %[[VAL_406]], label %[[VAL_407:.*]], label %[[VAL_408:.*]]
// CHECK:       output_x_in_tile-after76:                         ; preds = %[[VAL_407]], %[[VAL_403]]
// CHECK:         %[[VAL_409:.*]] = add i32 192, %[[VAL_135]]
// CHECK:         %[[VAL_410:.*]] = add i32 %[[VAL_136]], 192
// CHECK:         %[[VAL_411:.*]] = icmp ult i32 %[[VAL_409]], %[[VAL_132]]
// CHECK:         br i1 %[[VAL_411]], label %[[VAL_412:.*]], label %[[VAL_413:.*]]
// CHECK:       output_x_in_tile-after83:                         ; preds = %[[VAL_412]], %[[VAL_408]]
// CHECK:         %[[VAL_414:.*]] = add i32 193, %[[VAL_135]]
// CHECK:         %[[VAL_415:.*]] = add i32 %[[VAL_136]], 193
// CHECK:         %[[VAL_416:.*]] = icmp ult i32 %[[VAL_414]], %[[VAL_132]]
// CHECK:         br i1 %[[VAL_416]], label %[[VAL_417:.*]], label %[[VAL_154]]
// CHECK:       output_x_in_tile-after90:                         ; preds = %[[VAL_417]], %[[VAL_413]]
// CHECK:         br label %[[VAL_142]]
// CHECK:       output_x_in_tile-true:                            ; preds = %[[VAL_153]]
// CHECK:         %[[VAL_418:.*]] = mul nuw nsw i32 %[[VAL_380]], 1
// CHECK:         %[[VAL_419:.*]] = add nuw nsw i32 0, %[[VAL_418]]
// CHECK:         %[[VAL_420:.*]] = mul nuw nsw i32 %[[VAL_378]], 32
// CHECK:         %[[VAL_421:.*]] = add nuw nsw i32 %[[VAL_419]], %[[VAL_420]]
// CHECK:         %[[VAL_422:.*]] = mul nuw nsw i32 %[[VAL_128]], 2048
// CHECK:         %[[VAL_423:.*]] = add nuw nsw i32 %[[VAL_421]], %[[VAL_422]]
// CHECK:         %[[VAL_424:.*]] = udiv i32 %[[VAL_423]], 1
// CHECK:         %[[VAL_425:.*]] = urem i32 %[[VAL_424]], 32
// CHECK:         %[[VAL_426:.*]] = udiv i32 %[[VAL_423]], 32
// CHECK:         %[[VAL_427:.*]] = urem i32 %[[VAL_426]], 32
// CHECK:         %[[VAL_428:.*]] = udiv i32 %[[VAL_423]], 1024
// CHECK:         %[[VAL_429:.*]] = bitcast [2 x [32 x [32 x float]]]* %[[VAL_95]] to float*
// CHECK:         %[[VAL_430:.*]] = getelementptr inbounds float, float* %[[VAL_429]], i32 %[[VAL_423]]
// CHECK:         %[[VAL_431:.*]] = load float, float* %[[VAL_430]], align 4, !invariant.load !8
// CHECK:         store float %[[VAL_431]], float* %[[VAL_92]], align 4
// CHECK:         %[[VAL_432:.*]] = getelementptr inbounds float, float* %[[VAL_91]], i32 0
// CHECK:         call void @region_1_4(float* %[[VAL_432]], float* %[[VAL_92]], float* %[[VAL_432]])
// CHECK:         %[[VAL_433:.*]] = bitcast [2 x [32 x [32 x float]]]* %[[VAL_95]] to float*
// CHECK:         %[[VAL_434:.*]] = getelementptr inbounds float, float* %[[VAL_433]], i32 %[[VAL_423]]
// CHECK:         %[[VAL_435:.*]] = load float, float* %[[VAL_434]], align 4, !invariant.load !8
// CHECK:         store float %[[VAL_435]], float* %[[VAL_90]], align 4
// CHECK:         %[[VAL_436:.*]] = getelementptr inbounds float, float* %[[VAL_89]], i32 0
// CHECK:         call void @region_2_9(float* %[[VAL_436]], float* %[[VAL_90]], float* %[[VAL_436]])
// CHECK:         br label %[[VAL_383]]
// CHECK:       output_x_in_tile-true47:                          ; preds = %[[VAL_383]]
// CHECK:         %[[VAL_437:.*]] = mul nuw nsw i32 %[[VAL_385]], 1
// CHECK:         %[[VAL_438:.*]] = add nuw nsw i32 0, %[[VAL_437]]
// CHECK:         %[[VAL_439:.*]] = mul nuw nsw i32 %[[VAL_378]], 32
// CHECK:         %[[VAL_440:.*]] = add nuw nsw i32 %[[VAL_438]], %[[VAL_439]]
// CHECK:         %[[VAL_441:.*]] = mul nuw nsw i32 %[[VAL_128]], 2048
// CHECK:         %[[VAL_442:.*]] = add nuw nsw i32 %[[VAL_440]], %[[VAL_441]]
// CHECK:         %[[VAL_443:.*]] = udiv i32 %[[VAL_442]], 1
// CHECK:         %[[VAL_444:.*]] = urem i32 %[[VAL_443]], 32
// CHECK:         %[[VAL_445:.*]] = udiv i32 %[[VAL_442]], 32
// CHECK:         %[[VAL_446:.*]] = urem i32 %[[VAL_445]], 32
// CHECK:         %[[VAL_447:.*]] = udiv i32 %[[VAL_442]], 1024
// CHECK:         %[[VAL_448:.*]] = bitcast [2 x [32 x [32 x float]]]* %[[VAL_95]] to float*
// CHECK:         %[[VAL_449:.*]] = getelementptr inbounds float, float* %[[VAL_448]], i32 %[[VAL_442]]
// CHECK:         %[[VAL_450:.*]] = load float, float* %[[VAL_449]], align 4, !invariant.load !8
// CHECK:         store float %[[VAL_450]], float* %[[VAL_92]], align 4
// CHECK:         %[[VAL_451:.*]] = getelementptr inbounds float, float* %[[VAL_91]], i32 0
// CHECK:         call void @region_1_4(float* %[[VAL_451]], float* %[[VAL_92]], float* %[[VAL_451]])
// CHECK:         %[[VAL_452:.*]] = bitcast [2 x [32 x [32 x float]]]* %[[VAL_95]] to float*
// CHECK:         %[[VAL_453:.*]] = getelementptr inbounds float, float* %[[VAL_452]], i32 %[[VAL_442]]
// CHECK:         %[[VAL_454:.*]] = load float, float* %[[VAL_453]], align 4, !invariant.load !8
// CHECK:         store float %[[VAL_454]], float* %[[VAL_90]], align 4
// CHECK:         %[[VAL_455:.*]] = getelementptr inbounds float, float* %[[VAL_89]], i32 0
// CHECK:         call void @region_2_9(float* %[[VAL_455]], float* %[[VAL_90]], float* %[[VAL_455]])
// CHECK:         br label %[[VAL_388]]
// CHECK:       output_x_in_tile-true54:                          ; preds = %[[VAL_388]]
// CHECK:         %[[VAL_456:.*]] = mul nuw nsw i32 %[[VAL_390]], 1
// CHECK:         %[[VAL_457:.*]] = add nuw nsw i32 0, %[[VAL_456]]
// CHECK:         %[[VAL_458:.*]] = mul nuw nsw i32 %[[VAL_378]], 32
// CHECK:         %[[VAL_459:.*]] = add nuw nsw i32 %[[VAL_457]], %[[VAL_458]]
// CHECK:         %[[VAL_460:.*]] = mul nuw nsw i32 %[[VAL_128]], 2048
// CHECK:         %[[VAL_461:.*]] = add nuw nsw i32 %[[VAL_459]], %[[VAL_460]]
// CHECK:         %[[VAL_462:.*]] = udiv i32 %[[VAL_461]], 1
// CHECK:         %[[VAL_463:.*]] = urem i32 %[[VAL_462]], 32
// CHECK:         %[[VAL_464:.*]] = udiv i32 %[[VAL_461]], 32
// CHECK:         %[[VAL_465:.*]] = urem i32 %[[VAL_464]], 32
// CHECK:         %[[VAL_466:.*]] = udiv i32 %[[VAL_461]], 1024
// CHECK:         %[[VAL_467:.*]] = bitcast [2 x [32 x [32 x float]]]* %[[VAL_95]] to float*
// CHECK:         %[[VAL_468:.*]] = getelementptr inbounds float, float* %[[VAL_467]], i32 %[[VAL_461]]
// CHECK:         %[[VAL_469:.*]] = load float, float* %[[VAL_468]], align 4, !invariant.load !8
// CHECK:         store float %[[VAL_469]], float* %[[VAL_92]], align 4
// CHECK:         %[[VAL_470:.*]] = getelementptr inbounds float, float* %[[VAL_91]], i32 0
// CHECK:         call void @region_1_4(float* %[[VAL_470]], float* %[[VAL_92]], float* %[[VAL_470]])
// CHECK:         %[[VAL_471:.*]] = bitcast [2 x [32 x [32 x float]]]* %[[VAL_95]] to float*
// CHECK:         %[[VAL_472:.*]] = getelementptr inbounds float, float* %[[VAL_471]], i32 %[[VAL_461]]
// CHECK:         %[[VAL_473:.*]] = load float, float* %[[VAL_472]], align 4, !invariant.load !8
// CHECK:         store float %[[VAL_473]], float* %[[VAL_90]], align 4
// CHECK:         %[[VAL_474:.*]] = getelementptr inbounds float, float* %[[VAL_89]], i32 0
// CHECK:         call void @region_2_9(float* %[[VAL_474]], float* %[[VAL_90]], float* %[[VAL_474]])
// CHECK:         br label %[[VAL_393]]
// CHECK:       output_x_in_tile-true61:                          ; preds = %[[VAL_393]]
// CHECK:         %[[VAL_475:.*]] = mul nuw nsw i32 %[[VAL_395]], 1
// CHECK:         %[[VAL_476:.*]] = add nuw nsw i32 0, %[[VAL_475]]
// CHECK:         %[[VAL_477:.*]] = mul nuw nsw i32 %[[VAL_378]], 32
// CHECK:         %[[VAL_478:.*]] = add nuw nsw i32 %[[VAL_476]], %[[VAL_477]]
// CHECK:         %[[VAL_479:.*]] = mul nuw nsw i32 %[[VAL_128]], 2048
// CHECK:         %[[VAL_480:.*]] = add nuw nsw i32 %[[VAL_478]], %[[VAL_479]]
// CHECK:         %[[VAL_481:.*]] = udiv i32 %[[VAL_480]], 1
// CHECK:         %[[VAL_482:.*]] = urem i32 %[[VAL_481]], 32
// CHECK:         %[[VAL_483:.*]] = udiv i32 %[[VAL_480]], 32
// CHECK:         %[[VAL_484:.*]] = urem i32 %[[VAL_483]], 32
// CHECK:         %[[VAL_485:.*]] = udiv i32 %[[VAL_480]], 1024
// CHECK:         %[[VAL_486:.*]] = bitcast [2 x [32 x [32 x float]]]* %[[VAL_95]] to float*
// CHECK:         %[[VAL_487:.*]] = getelementptr inbounds float, float* %[[VAL_486]], i32 %[[VAL_480]]
// CHECK:         %[[VAL_488:.*]] = load float, float* %[[VAL_487]], align 4, !invariant.load !8
// CHECK:         store float %[[VAL_488]], float* %[[VAL_92]], align 4
// CHECK:         %[[VAL_489:.*]] = getelementptr inbounds float, float* %[[VAL_91]], i32 0
// CHECK:         call void @region_1_4(float* %[[VAL_489]], float* %[[VAL_92]], float* %[[VAL_489]])
// CHECK:         %[[VAL_490:.*]] = bitcast [2 x [32 x [32 x float]]]* %[[VAL_95]] to float*
// CHECK:         %[[VAL_491:.*]] = getelementptr inbounds float, float* %[[VAL_490]], i32 %[[VAL_480]]
// CHECK:         %[[VAL_492:.*]] = load float, float* %[[VAL_491]], align 4, !invariant.load !8
// CHECK:         store float %[[VAL_492]], float* %[[VAL_90]], align 4
// CHECK:         %[[VAL_493:.*]] = getelementptr inbounds float, float* %[[VAL_89]], i32 0
// CHECK:         call void @region_2_9(float* %[[VAL_493]], float* %[[VAL_90]], float* %[[VAL_493]])
// CHECK:         br label %[[VAL_398]]
// CHECK:       output_x_in_tile-true68:                          ; preds = %[[VAL_398]]
// CHECK:         %[[VAL_494:.*]] = mul nuw nsw i32 %[[VAL_400]], 1
// CHECK:         %[[VAL_495:.*]] = add nuw nsw i32 0, %[[VAL_494]]
// CHECK:         %[[VAL_496:.*]] = mul nuw nsw i32 %[[VAL_378]], 32
// CHECK:         %[[VAL_497:.*]] = add nuw nsw i32 %[[VAL_495]], %[[VAL_496]]
// CHECK:         %[[VAL_498:.*]] = mul nuw nsw i32 %[[VAL_128]], 2048
// CHECK:         %[[VAL_499:.*]] = add nuw nsw i32 %[[VAL_497]], %[[VAL_498]]
// CHECK:         %[[VAL_500:.*]] = udiv i32 %[[VAL_499]], 1
// CHECK:         %[[VAL_501:.*]] = urem i32 %[[VAL_500]], 32
// CHECK:         %[[VAL_502:.*]] = udiv i32 %[[VAL_499]], 32
// CHECK:         %[[VAL_503:.*]] = urem i32 %[[VAL_502]], 32
// CHECK:         %[[VAL_504:.*]] = udiv i32 %[[VAL_499]], 1024
// CHECK:         %[[VAL_505:.*]] = bitcast [2 x [32 x [32 x float]]]* %[[VAL_95]] to float*
// CHECK:         %[[VAL_506:.*]] = getelementptr inbounds float, float* %[[VAL_505]], i32 %[[VAL_499]]
// CHECK:         %[[VAL_507:.*]] = load float, float* %[[VAL_506]], align 4, !invariant.load !8
// CHECK:         store float %[[VAL_507]], float* %[[VAL_92]], align 4
// CHECK:         %[[VAL_508:.*]] = getelementptr inbounds float, float* %[[VAL_91]], i32 0
// CHECK:         call void @region_1_4(float* %[[VAL_508]], float* %[[VAL_92]], float* %[[VAL_508]])
// CHECK:         %[[VAL_509:.*]] = bitcast [2 x [32 x [32 x float]]]* %[[VAL_95]] to float*
// CHECK:         %[[VAL_510:.*]] = getelementptr inbounds float, float* %[[VAL_509]], i32 %[[VAL_499]]
// CHECK:         %[[VAL_511:.*]] = load float, float* %[[VAL_510]], align 4, !invariant.load !8
// CHECK:         store float %[[VAL_511]], float* %[[VAL_90]], align 4
// CHECK:         %[[VAL_512:.*]] = getelementptr inbounds float, float* %[[VAL_89]], i32 0
// CHECK:         call void @region_2_9(float* %[[VAL_512]], float* %[[VAL_90]], float* %[[VAL_512]])
// CHECK:         br label %[[VAL_403]]
// CHECK:       output_x_in_tile-true75:                          ; preds = %[[VAL_403]]
// CHECK:         %[[VAL_513:.*]] = mul nuw nsw i32 %[[VAL_405]], 1
// CHECK:         %[[VAL_514:.*]] = add nuw nsw i32 0, %[[VAL_513]]
// CHECK:         %[[VAL_515:.*]] = mul nuw nsw i32 %[[VAL_378]], 32
// CHECK:         %[[VAL_516:.*]] = add nuw nsw i32 %[[VAL_514]], %[[VAL_515]]
// CHECK:         %[[VAL_517:.*]] = mul nuw nsw i32 %[[VAL_128]], 2048
// CHECK:         %[[VAL_518:.*]] = add nuw nsw i32 %[[VAL_516]], %[[VAL_517]]
// CHECK:         %[[VAL_519:.*]] = udiv i32 %[[VAL_518]], 1
// CHECK:         %[[VAL_520:.*]] = urem i32 %[[VAL_519]], 32
// CHECK:         %[[VAL_521:.*]] = udiv i32 %[[VAL_518]], 32
// CHECK:         %[[VAL_522:.*]] = urem i32 %[[VAL_521]], 32
// CHECK:         %[[VAL_523:.*]] = udiv i32 %[[VAL_518]], 1024
// CHECK:         %[[VAL_524:.*]] = bitcast [2 x [32 x [32 x float]]]* %[[VAL_95]] to float*
// CHECK:         %[[VAL_525:.*]] = getelementptr inbounds float, float* %[[VAL_524]], i32 %[[VAL_518]]
// CHECK:         %[[VAL_526:.*]] = load float, float* %[[VAL_525]], align 4, !invariant.load !8
// CHECK:         store float %[[VAL_526]], float* %[[VAL_92]], align 4
// CHECK:         %[[VAL_527:.*]] = getelementptr inbounds float, float* %[[VAL_91]], i32 0
// CHECK:         call void @region_1_4(float* %[[VAL_527]], float* %[[VAL_92]], float* %[[VAL_527]])
// CHECK:         %[[VAL_528:.*]] = bitcast [2 x [32 x [32 x float]]]* %[[VAL_95]] to float*
// CHECK:         %[[VAL_529:.*]] = getelementptr inbounds float, float* %[[VAL_528]], i32 %[[VAL_518]]
// CHECK:         %[[VAL_530:.*]] = load float, float* %[[VAL_529]], align 4, !invariant.load !8
// CHECK:         store float %[[VAL_530]], float* %[[VAL_90]], align 4
// CHECK:         %[[VAL_531:.*]] = getelementptr inbounds float, float* %[[VAL_89]], i32 0
// CHECK:         call void @region_2_9(float* %[[VAL_531]], float* %[[VAL_90]], float* %[[VAL_531]])
// CHECK:         br label %[[VAL_408]]
// CHECK:       output_x_in_tile-true82:                          ; preds = %[[VAL_408]]
// CHECK:         %[[VAL_532:.*]] = mul nuw nsw i32 %[[VAL_410]], 1
// CHECK:         %[[VAL_533:.*]] = add nuw nsw i32 0, %[[VAL_532]]
// CHECK:         %[[VAL_534:.*]] = mul nuw nsw i32 %[[VAL_378]], 32
// CHECK:         %[[VAL_535:.*]] = add nuw nsw i32 %[[VAL_533]], %[[VAL_534]]
// CHECK:         %[[VAL_536:.*]] = mul nuw nsw i32 %[[VAL_128]], 2048
// CHECK:         %[[VAL_537:.*]] = add nuw nsw i32 %[[VAL_535]], %[[VAL_536]]
// CHECK:         %[[VAL_538:.*]] = udiv i32 %[[VAL_537]], 1
// CHECK:         %[[VAL_539:.*]] = urem i32 %[[VAL_538]], 32
// CHECK:         %[[VAL_540:.*]] = udiv i32 %[[VAL_537]], 32
// CHECK:         %[[VAL_541:.*]] = urem i32 %[[VAL_540]], 32
// CHECK:         %[[VAL_542:.*]] = udiv i32 %[[VAL_537]], 1024
// CHECK:         %[[VAL_543:.*]] = bitcast [2 x [32 x [32 x float]]]* %[[VAL_95]] to float*
// CHECK:         %[[VAL_544:.*]] = getelementptr inbounds float, float* %[[VAL_543]], i32 %[[VAL_537]]
// CHECK:         %[[VAL_545:.*]] = load float, float* %[[VAL_544]], align 4, !invariant.load !8
// CHECK:         store float %[[VAL_545]], float* %[[VAL_92]], align 4
// CHECK:         %[[VAL_546:.*]] = getelementptr inbounds float, float* %[[VAL_91]], i32 0
// CHECK:         call void @region_1_4(float* %[[VAL_546]], float* %[[VAL_92]], float* %[[VAL_546]])
// CHECK:         %[[VAL_547:.*]] = bitcast [2 x [32 x [32 x float]]]* %[[VAL_95]] to float*
// CHECK:         %[[VAL_548:.*]] = getelementptr inbounds float, float* %[[VAL_547]], i32 %[[VAL_537]]
// CHECK:         %[[VAL_549:.*]] = load float, float* %[[VAL_548]], align 4, !invariant.load !8
// CHECK:         store float %[[VAL_549]], float* %[[VAL_90]], align 4
// CHECK:         %[[VAL_550:.*]] = getelementptr inbounds float, float* %[[VAL_89]], i32 0
// CHECK:         call void @region_2_9(float* %[[VAL_550]], float* %[[VAL_90]], float* %[[VAL_550]])
// CHECK:         br label %[[VAL_413]]
// CHECK:       output_x_in_tile-true89:                          ; preds = %[[VAL_413]]
// CHECK:         %[[VAL_551:.*]] = mul nuw nsw i32 %[[VAL_415]], 1
// CHECK:         %[[VAL_552:.*]] = add nuw nsw i32 0, %[[VAL_551]]
// CHECK:         %[[VAL_553:.*]] = mul nuw nsw i32 %[[VAL_378]], 32
// CHECK:         %[[VAL_554:.*]] = add nuw nsw i32 %[[VAL_552]], %[[VAL_553]]
// CHECK:         %[[VAL_555:.*]] = mul nuw nsw i32 %[[VAL_128]], 2048
// CHECK:         %[[VAL_556:.*]] = add nuw nsw i32 %[[VAL_554]], %[[VAL_555]]
// CHECK:         %[[VAL_557:.*]] = udiv i32 %[[VAL_556]], 1
// CHECK:         %[[VAL_558:.*]] = urem i32 %[[VAL_557]], 32
// CHECK:         %[[VAL_559:.*]] = udiv i32 %[[VAL_556]], 32
// CHECK:         %[[VAL_560:.*]] = urem i32 %[[VAL_559]], 32
// CHECK:         %[[VAL_561:.*]] = udiv i32 %[[VAL_556]], 1024
// CHECK:         %[[VAL_562:.*]] = bitcast [2 x [32 x [32 x float]]]* %[[VAL_95]] to float*
// CHECK:         %[[VAL_563:.*]] = getelementptr inbounds float, float* %[[VAL_562]], i32 %[[VAL_556]]
// CHECK:         %[[VAL_564:.*]] = load float, float* %[[VAL_563]], align 4, !invariant.load !8
// CHECK:         store float %[[VAL_564]], float* %[[VAL_92]], align 4
// CHECK:         %[[VAL_565:.*]] = getelementptr inbounds float, float* %[[VAL_91]], i32 0
// CHECK:         call void @region_1_4(float* %[[VAL_565]], float* %[[VAL_92]], float* %[[VAL_565]])
// CHECK:         %[[VAL_566:.*]] = bitcast [2 x [32 x [32 x float]]]* %[[VAL_95]] to float*
// CHECK:         %[[VAL_567:.*]] = getelementptr inbounds float, float* %[[VAL_566]], i32 %[[VAL_556]]
// CHECK:         %[[VAL_568:.*]] = load float, float* %[[VAL_567]], align 4, !invariant.load !8
// CHECK:         store float %[[VAL_568]], float* %[[VAL_90]], align 4
// CHECK:         %[[VAL_569:.*]] = getelementptr inbounds float, float* %[[VAL_89]], i32 0
// CHECK:         call void @region_2_9(float* %[[VAL_569]], float* %[[VAL_90]], float* %[[VAL_569]])
// CHECK:         br label %[[VAL_154]]
// CHECK:       intra_warp_reduce_write-true:                     ; preds = %[[VAL_145]]
// CHECK:         %[[VAL_570:.*]] = getelementptr inbounds [1 x [32 x float]], [1 x [32 x float]] addrspace(3)* @shared_cache_0, i32 0, i32 0, i32 %[[VAL_178]]
// CHECK:         %[[VAL_571:.*]] = addrspacecast float addrspace(3)* %[[VAL_570]] to float*
// CHECK:         %[[VAL_572:.*]] = load float, float* %[[VAL_167]], align 4
// CHECK:         store float %[[VAL_572]], float* %[[VAL_571]], align 4
// CHECK:         br label %[[VAL_181]]
// CHECK:       inter_warp_reduce-true:                           ; preds = %[[VAL_181]]
// CHECK:         %[[VAL_573:.*]] = getelementptr inbounds [1 x [32 x float]], [1 x [32 x float]] addrspace(3)* @shared_cache_0, i32 0, i32 0, i32 %[[VAL_158]]
// CHECK:         %[[VAL_574:.*]] = addrspacecast float addrspace(3)* %[[VAL_573]] to float*
// CHECK:         store float %[[VAL_114]], float* %[[VAL_82]], align 4
// CHECK:         %[[VAL_575:.*]] = icmp ult i32 %[[VAL_156]], 1
// CHECK:         %[[VAL_576:.*]] = select i1 %[[VAL_575]], float* %[[VAL_574]], float* %[[VAL_82]]
// CHECK:         %[[VAL_577:.*]] = load float, float* %[[VAL_576]], align 4
// CHECK:         %[[VAL_578:.*]] = call float @llvm.nvvm.shfl.sync.down.f32(i32 -1, float %[[VAL_577]], i32 16, i32 31)
// CHECK:         store float %[[VAL_578]], float* %[[VAL_81]], align 4
// CHECK:         call void @region_1_4(float* %[[VAL_576]], float* %[[VAL_81]], float* %[[VAL_576]])
// CHECK:         %[[VAL_579:.*]] = load float, float* %[[VAL_576]], align 4
// CHECK:         %[[VAL_580:.*]] = call float @llvm.nvvm.shfl.sync.down.f32(i32 -1, float %[[VAL_579]], i32 8, i32 31)
// CHECK:         store float %[[VAL_580]], float* %[[VAL_80]], align 4
// CHECK:         call void @region_1_4(float* %[[VAL_576]], float* %[[VAL_80]], float* %[[VAL_576]])
// CHECK:         %[[VAL_581:.*]] = load float, float* %[[VAL_576]], align 4
// CHECK:         %[[VAL_582:.*]] = call float @llvm.nvvm.shfl.sync.down.f32(i32 -1, float %[[VAL_581]], i32 4, i32 31)
// CHECK:         store float %[[VAL_582]], float* %[[VAL_79]], align 4
// CHECK:         call void @region_1_4(float* %[[VAL_576]], float* %[[VAL_79]], float* %[[VAL_576]])
// CHECK:         %[[VAL_583:.*]] = load float, float* %[[VAL_576]], align 4
// CHECK:         %[[VAL_584:.*]] = call float @llvm.nvvm.shfl.sync.down.f32(i32 -1, float %[[VAL_583]], i32 2, i32 31)
// CHECK:         store float %[[VAL_584]], float* %[[VAL_78]], align 4
// CHECK:         call void @region_1_4(float* %[[VAL_576]], float* %[[VAL_78]], float* %[[VAL_576]])
// CHECK:         %[[VAL_585:.*]] = load float, float* %[[VAL_576]], align 4
// CHECK:         %[[VAL_586:.*]] = call float @llvm.nvvm.shfl.sync.down.f32(i32 -1, float %[[VAL_585]], i32 1, i32 31)
// CHECK:         store float %[[VAL_586]], float* %[[VAL_77]], align 4
// CHECK:         call void @region_1_4(float* %[[VAL_576]], float* %[[VAL_77]], float* %[[VAL_576]])
// CHECK:         %[[VAL_587:.*]] = icmp eq i32 %[[VAL_156]], 0
// CHECK:         br i1 %[[VAL_587]], label %[[VAL_588:.*]], label %[[VAL_185]]
// CHECK:       reduction_atomic_update-after:                    ; preds = %[[VAL_588]], %[[VAL_183]]
// CHECK:         br label %[[VAL_184]]
// CHECK:       reduction_atomic_update-true:                     ; preds = %[[VAL_183]]
// CHECK:         %[[VAL_589:.*]] = load float, float* %[[VAL_574]], align 4
// CHECK:         %[[VAL_590:.*]] = atomicrmw fadd float* %[[VAL_166]], float %[[VAL_589]] seq_cst, align 4
// CHECK:         br label %[[VAL_185]]
// CHECK:       intra_warp_reduce_write-true128:                  ; preds = %[[VAL_184]]
// CHECK:         %[[VAL_591:.*]] = getelementptr inbounds [1 x [32 x float]], [1 x [32 x float]] addrspace(3)* @shared_cache_1, i32 0, i32 0, i32 %[[VAL_202]]
// CHECK:         %[[VAL_592:.*]] = addrspacecast float addrspace(3)* %[[VAL_591]] to float*
// CHECK:         %[[VAL_593:.*]] = load float, float* %[[VAL_191]], align 4
// CHECK:         store float %[[VAL_593]], float* %[[VAL_592]], align 4
// CHECK:         br label %[[VAL_205]]
// CHECK:       inter_warp_reduce-true130:                        ; preds = %[[VAL_205]]
// CHECK:         %[[VAL_594:.*]] = getelementptr inbounds [1 x [32 x float]], [1 x [32 x float]] addrspace(3)* @shared_cache_1, i32 0, i32 0, i32 %[[VAL_158]]
// CHECK:         %[[VAL_595:.*]] = addrspacecast float addrspace(3)* %[[VAL_594]] to float*
// CHECK:         store float %[[VAL_116]], float* %[[VAL_71]], align 4
// CHECK:         %[[VAL_596:.*]] = icmp ult i32 %[[VAL_156]], 1
// CHECK:         %[[VAL_597:.*]] = select i1 %[[VAL_596]], float* %[[VAL_595]], float* %[[VAL_71]]
// CHECK:         %[[VAL_598:.*]] = load float, float* %[[VAL_597]], align 4
// CHECK:         %[[VAL_599:.*]] = call float @llvm.nvvm.shfl.sync.down.f32(i32 -1, float %[[VAL_598]], i32 16, i32 31)
// CHECK:         store float %[[VAL_599]], float* %[[VAL_70]], align 4
// CHECK:         call void @region_2_9(float* %[[VAL_597]], float* %[[VAL_70]], float* %[[VAL_597]])
// CHECK:         %[[VAL_600:.*]] = load float, float* %[[VAL_597]], align 4
// CHECK:         %[[VAL_601:.*]] = call float @llvm.nvvm.shfl.sync.down.f32(i32 -1, float %[[VAL_600]], i32 8, i32 31)
// CHECK:         store float %[[VAL_601]], float* %[[VAL_69]], align 4
// CHECK:         call void @region_2_9(float* %[[VAL_597]], float* %[[VAL_69]], float* %[[VAL_597]])
// CHECK:         %[[VAL_602:.*]] = load float, float* %[[VAL_597]], align 4
// CHECK:         %[[VAL_603:.*]] = call float @llvm.nvvm.shfl.sync.down.f32(i32 -1, float %[[VAL_602]], i32 4, i32 31)
// CHECK:         store float %[[VAL_603]], float* %[[VAL_68]], align 4
// CHECK:         call void @region_2_9(float* %[[VAL_597]], float* %[[VAL_68]], float* %[[VAL_597]])
// CHECK:         %[[VAL_604:.*]] = load float, float* %[[VAL_597]], align 4
// CHECK:         %[[VAL_605:.*]] = call float @llvm.nvvm.shfl.sync.down.f32(i32 -1, float %[[VAL_604]], i32 2, i32 31)
// CHECK:         store float %[[VAL_605]], float* %[[VAL_67]], align 4
// CHECK:         call void @region_2_9(float* %[[VAL_597]], float* %[[VAL_67]], float* %[[VAL_597]])
// CHECK:         %[[VAL_606:.*]] = load float, float* %[[VAL_597]], align 4
// CHECK:         %[[VAL_607:.*]] = call float @llvm.nvvm.shfl.sync.down.f32(i32 -1, float %[[VAL_606]], i32 1, i32 31)
// CHECK:         store float %[[VAL_607]], float* %[[VAL_66]], align 4
// CHECK:         call void @region_2_9(float* %[[VAL_597]], float* %[[VAL_66]], float* %[[VAL_597]])
// CHECK:         %[[VAL_608:.*]] = icmp eq i32 %[[VAL_156]], 0
// CHECK:         br i1 %[[VAL_608]], label %[[VAL_609:.*]], label %[[VAL_208]]
// CHECK:       reduction_atomic_update-after144:                 ; preds = %[[VAL_610:.*]], %[[VAL_207]]
// CHECK:         br label %[[VAL_112]]
// CHECK:       reduction_atomic_update-true143:                  ; preds = %[[VAL_207]]
// CHECK:         %[[VAL_611:.*]] = load float, float* %[[VAL_595]], align 4
// CHECK:         %[[VAL_612:.*]] = bitcast float* %[[VAL_190]] to i32*
// CHECK:         %[[VAL_613:.*]] = bitcast i32* %[[VAL_64]] to float*
// CHECK:         %[[VAL_614:.*]] = load i32, i32* %[[VAL_612]], align 4
// CHECK:         store i32 %[[VAL_614]], i32* %[[VAL_65]], align 4
// CHECK:         br label %[[VAL_615:.*]]
// CHECK:       atomic_op_loop_exit:                              ; preds = %[[VAL_616:.*]], %[[VAL_615]]
// CHECK:         br label %[[VAL_208]]
// CHECK:       atomic_op_loop_body:                              ; preds = %[[VAL_616]], %[[VAL_609]]
// CHECK:         %[[VAL_617:.*]] = load i32, i32* %[[VAL_65]], align 4
// CHECK:         store i32 %[[VAL_617]], i32* %[[VAL_64]], align 4
// CHECK:         call void @region_2_9(float* %[[VAL_613]], float* %[[VAL_595]], float* %[[VAL_613]])
// CHECK:         %[[VAL_618:.*]] = load i32, i32* %[[VAL_64]], align 4
// CHECK:         %[[VAL_619:.*]] = icmp eq i32 %[[VAL_617]], %[[VAL_618]]
// CHECK:         br i1 %[[VAL_619]], label %[[VAL_610]], label %[[VAL_616]]
// CHECK:       atomic_op_loop_cas:                               ; preds = %[[VAL_615]]
// CHECK:         %[[VAL_620:.*]] = cmpxchg i32* %[[VAL_612]], i32 %[[VAL_617]], i32 %[[VAL_618]] seq_cst seq_cst, align 4
// CHECK:         %[[VAL_621:.*]] = extractvalue { i32, i1 } %[[VAL_620]], 0
// CHECK:         store i32 %[[VAL_621]], i32* %[[VAL_65]], align 4
// CHECK:         %[[VAL_622:.*]] = extractvalue { i32, i1 } %[[VAL_620]], 1
// CHECK:         br i1 %[[VAL_622]], label %[[VAL_610]], label %[[VAL_615]]

// CHECK: define internal void @region_1_4(float* dereferenceable(4) %[[VAL_0:.*]], float* dereferenceable(4) %[[VAL_1:.*]], float* dereferenceable(4) %[[VAL_2:.*]]) {
// CHECK-LABEL: entry:
// CHECK:         %[[VAL_623:.*]] = alloca float, align 4
// CHECK:         %[[VAL_624:.*]] = load float, float* %[[VAL_625:.*]], align 4
// CHECK:         %[[VAL_626:.*]] = load float, float* %[[VAL_627:.*]], align 4
// CHECK:         %[[VAL_628:.*]] = fadd float %[[VAL_624]], %[[VAL_626]]
// CHECK:         store float %[[VAL_628]], float* %[[VAL_623]], align 4
// CHECK:         %[[VAL_629:.*]] = load float, float* %[[VAL_623]], align 4
// CHECK:         store float %[[VAL_629]], float* %[[VAL_630:.*]], align 4
// CHECK:         ret void

// CHECK: define internal void @region_2_9(float* dereferenceable(4) %[[VAL_0:.*]], float* dereferenceable(4) %[[VAL_1:.*]], float* dereferenceable(4) %[[VAL_2:.*]]) {
// CHECK-LABEL: entry:
// CHECK:         %[[VAL_631:.*]] = alloca float, align 4
// CHECK:         %[[VAL_632:.*]] = load float, float* %[[VAL_633:.*]], align 4
// CHECK:         %[[VAL_634:.*]] = load float, float* %[[VAL_635:.*]], align 4
// CHECK:         %[[VAL_636:.*]] = call float @llvm.maxnum.f32(float %[[VAL_632]], float %[[VAL_634]])
// CHECK:         store float %[[VAL_636]], float* %[[VAL_631]], align 4
// CHECK:         %[[VAL_637:.*]] = load float, float* %[[VAL_631]], align 4
// CHECK:         store float %[[VAL_637]], float* %[[VAL_638:.*]], align 4
// CHECK:         ret void

HloModule Test

Add {
  lhsadd = f32[] parameter(0)
  rhsadd = f32[] parameter(1)
  ROOT add = f32[] add(lhsadd, rhsadd)
}

Max {
  lhsmax = f32[] parameter(0)
  rhsmax = f32[] parameter(1)
  ROOT max = f32[] maximum(lhsmax, rhsmax)
}


fused_reduce {
  p0 = f32[2,32,32]{2,1,0} parameter(0)
  init1 = f32[] parameter(1)
  init2 = f32[] parameter(2)
  r1 = f32[2,32]{1,0} reduce(p0, init1), dimensions={2}, to_apply=Add
  r2 = f32[2,32]{1,0} reduce(p0, init2), dimensions={2}, to_apply=Max
  ROOT tuple = (f32[2,32]{1,0}, f32[2,32]{1,0}) tuple(r1, r2)
}

ENTRY reduce {
  p = f32[2,32,32]{2,1,0} parameter(0)
  i = f32[] parameter(1)
  j = f32[] parameter(2)
  ROOT fusion = (f32[2,32]{1,0}, f32[2,32]{1,0}) fusion(p, i, j),
   kind=kInput, calls=fused_reduce
}
